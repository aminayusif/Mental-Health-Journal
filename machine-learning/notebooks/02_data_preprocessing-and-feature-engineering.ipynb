{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5019ad6e",
   "metadata": {},
   "source": [
    "## Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358934d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f765cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# 1. DOWNLOAD NLTK RESOURCES\n",
    "# --------------------------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7c98d",
   "metadata": {},
   "source": [
    "## Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da02772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: D:\\project Github\\web dev + machine learning\\mental-health-journal\\machine-learning\\data\\cleaned\\mental_health_journal_cleaned.csv\n",
      "                                                text     mood  mood_score  \\\n",
      "0     Today I practiced mindfulness and felt calmer.    happy           8   \n",
      "1  I felt low today. Missed some important calls ...      sad           3   \n",
      "2  I was worried about the upcoming presentation,...  anxious           4   \n",
      "3  Today was a great day! I went jogging and felt...    happy           9   \n",
      "4  A normal day, went through routine tasks witho...  neutral           6   \n",
      "\n",
      "                                   tags   category                  created_at  \n",
      "0               [\"mindfulness\", \"calm\"]  Self-care  2025-09-22 19:46:41.691538  \n",
      "1                [\"work\", \"motivation\"]       Work  2025-09-22 21:56:26.714902  \n",
      "2            [\"presentation\", \"stress\"]     Career  2025-09-22 21:56:56.809605  \n",
      "3  [\"exercise\", \"wellness\", \"outdoors\"]     Health  2025-09-22 22:00:23.950201  \n",
      "4                  [\"routine\", \"daily\"]   Personal  2025-09-22 22:01:43.580868  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Current directory is notebooks/\n",
    "notebook_dir = Path().resolve()\n",
    "\n",
    "# Move up one level to machine-learning/\n",
    "ml_dir = notebook_dir.parent\n",
    "\n",
    "# Build path to the processed CSV\n",
    "data_path = ml_dir / \"data\" / \"cleaned\" / \"mental_health_journal_cleaned.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded data from: {data_path}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586da79",
   "metadata": {},
   "source": [
    "## Data Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af6dae",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "\n",
    "We perform text normalization, including lowercasing, removing punctuation, and removing extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110a7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 2. TOKENIZATION\n",
    "# --------------------------------------\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(nltk.word_tokenize)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. REMOVE STOPWORDS\n",
    "# --------------------------------------\n",
    "df[\"tokens_no_stop\"] = df[\"tokens\"].apply(\n",
    "    lambda tokens: [t for t in tokens if t not in stop_words]\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. LEMMATIZATION\n",
    "# --------------------------------------\n",
    "df[\"lemmatized\"] = df[\"tokens_no_stop\"].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(t) for t in tokens]\n",
    ")\n",
    "df[\"lemmatized_text\"] = df[\"lemmatized\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# --------------------------------------\n",
    "# 5. SENTIMENT LABELS\n",
    "# --------------------------------------\n",
    "def get_sentiment_label(text):\n",
    "    score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"clean_text\"].apply(get_sentiment_label)\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. VECTORIZATION (TF-IDF)\n",
    "# --------------------------------------\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"lemmatized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abe6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        10 non-null     object\n",
      " 1   mood        10 non-null     object\n",
      " 2   mood_score  10 non-null     int64 \n",
      " 3   tags        10 non-null     object\n",
      " 4   category    10 non-null     object\n",
      " 5   created_at  10 non-null     object\n",
      " 6   clean_text  10 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 692.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "#  TEXT NORMALIZATION\n",
    "# --------------------------------------\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077f962",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Split text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "#  TOKENIZATION\n",
    "# --------------------------------------\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54502b5",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Remove frequent but meaningless words (\"the\", \"and\", \"is\"...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  REMOVE STOPWORDS\n",
    "# --------------------------------------\n",
    "df[\"tokens_no_stop\"] = df[\"tokens\"].apply(\n",
    "    lambda tokens: [t for t in tokens if t not in stop_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd09aa",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Convert words to their root form (e.g., running → run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 5. LEMMATIZATION\n",
    "# --------------------------------------\n",
    "df[\"lemmatized\"] = df[\"tokens_no_stop\"].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(t) for t in tokens]\n",
    ")\n",
    "# Join back to text\n",
    "df[\"lemmatized_text\"] = df[\"lemmatized\"].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e35d3",
   "metadata": {},
   "source": [
    "### Sentiment Labels \n",
    "\n",
    "We can generate:\n",
    "\n",
    "VADER sentiment scores\n",
    "\n",
    "Convert to labels: positive, neutral, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# SENTIMENT LABELS\n",
    "# --------------------------------------\n",
    "def get_sentiment_label(text):\n",
    "    score = sia.polarity_scores(text)[\"compound\"]\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"clean_text\"].apply(get_sentiment_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262b400",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Transform text into numeric representation:\n",
    "\n",
    "TF-IDF or\n",
    "\n",
    "Transformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc2f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anything' 'balancing' 'better' 'call' 'calmer' 'chatting' 'couldnt'\n",
      " 'day' 'done' 'energized' 'family' 'feeling' 'felt' 'find' 'focus' 'get'\n",
      " 'good' 'great' 'happening' 'important' 'ive' 'jogging' 'life' 'low'\n",
      " 'mindfulness' 'missed' 'much' 'need' 'nice' 'normal' 'overwhelmed'\n",
      " 'personal' 'practiced' 'presentation' 'productive' 'progress' 'project'\n",
      " 'really' 'relaxing' 'routine' 'sleep' 'special' 'spent' 'struggled'\n",
      " 'struggling' 'task' 'time' 'today' 'unproductive' 'upcoming' 'well'\n",
      " 'went' 'without' 'work' 'working' 'worried']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "#  VECTORIZATION (TF-IDF)\n",
    "# --------------------------------------\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"lemmatized_text\"])\n",
    "\n",
    "# To view vocabulary:\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "\n",
    "# Get base directory relative to notebook\n",
    "base_dir = Path().resolve().parent  # since notebook is inside /notebooks/\n",
    "\n",
    "# Construct full save path\n",
    "processed_path = base_dir / \"data\" / \"processed\" / \"mental_health_journal_cleaned.csv\"\n",
    "\n",
    "# Save file\n",
    "df.to_csv(processed_path, index=False)\n",
    "print(f\"✅ Saved processed dataset to: {processed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
